{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split as sk_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "np.set_printoptions(threshold='nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9666, 140, 37)\n",
      "(9666,)\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.read_csv('datasets/tweets.csv', delimiter=',')\n",
    "testing_data = pd.read_csv('datasets/tweets2.csv', delimiter=',')\n",
    "\n",
    "clinton_data_1 = training_data[training_data.handle == \"HillaryClinton\"][\"text\"].as_matrix()\n",
    "trump_data_1 = training_data[training_data.handle == \"realDonaldTrump\"][\"text\"].as_matrix()\n",
    "\n",
    "clinton_data_2 = testing_data[testing_data.handle == \"HillaryClinton\"][\"text\"].as_matrix()\n",
    "trump_data_2 = testing_data[testing_data.handle == \"realDonaldTrump\"][\"text\"].as_matrix()\n",
    "\n",
    "clinton_data = np.concatenate((clinton_data_1, clinton_data_2), axis=0)\n",
    "trump_data = np.concatenate((trump_data_1, trump_data_2), axis=0)\n",
    "\n",
    "clinton_dataset = []\n",
    "trump_dataset = []\n",
    "\n",
    "def loadCharsFromTxt(text, dataset):\n",
    "    image = np.zeros((140, 37))\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(words): \n",
    "        if \"t.co\" in words[i]:\n",
    "            del words[i]\n",
    "        i += 1\n",
    "    \n",
    "    text = \" \".join(words)\n",
    "            \n",
    "    charCount = 0\n",
    "    for char in text:\n",
    "        index = None\n",
    "        if char.isalpha():\n",
    "            index = ord(char) - ord('a') + 1\n",
    "        elif char.isdigit():\n",
    "            index = ord(char) - ord('0') + 27\n",
    "        elif char == \" \":\n",
    "            index = 0\n",
    "                             \n",
    "        if index:\n",
    "            image[charCount, index] = 1\n",
    "            charCount += 1\n",
    "                    \n",
    "    dataset.append(image)\n",
    "\n",
    "for text in clinton_data:\n",
    "    loadCharsFromTxt(text, clinton_dataset)\n",
    "\n",
    "for text in trump_data:\n",
    "    loadCharsFromTxt(text, trump_dataset)\n",
    "    \n",
    "clinton_y = np.zeros(len(clinton_dataset))\n",
    "trump_y = np.full(len(trump_dataset), 1)\n",
    "\n",
    "x_data = np.concatenate((clinton_dataset, trump_dataset), axis=0)\n",
    "y_data = np.concatenate((clinton_y, trump_y), axis=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = sk_split(x_data, y_data, test_size = 0.25, random_state = 42)\n",
    "\n",
    "print x_train.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.997413614732\n",
      "0.98417132216\n"
     ]
    }
   ],
   "source": [
    "def loadDictionary(text, wc):\n",
    "    words = text.lower().split()\n",
    "    for word in words:\n",
    "        if \"t.co\" not in word:\n",
    "            i = 0\n",
    "            editedWord = word\n",
    "            while i < len(word):\n",
    "                if not word[i].isalpha() and not word[i].isdigit():\n",
    "                    editedWord = word[:i] + word[i+1:]\n",
    "                i += 1\n",
    "                \n",
    "            if word not in wordDictionary:\n",
    "                wordDictionary[word] = wc\n",
    "                wc += 1\n",
    "    \n",
    "    return wc\n",
    "                \n",
    "def bagOfWords(text, dataset, wc):\n",
    "    bag = np.zeros(wc)\n",
    "    for word in text.lower().split():\n",
    "        # ignore t.co links\n",
    "        if \"t.co\" not in word:\n",
    "            i = 0\n",
    "            \n",
    "            # ignore punctuation\n",
    "            editedWord = word\n",
    "            while i < len(word):\n",
    "                if not word[i].isalpha() and not word[i].isdigit():\n",
    "                    editedWord = word[:i] + word[i+1:]\n",
    "                i += 1\n",
    "\n",
    "            if word in wordDictionary:\n",
    "                bag[wordDictionary[word]] += 1\n",
    "    dataset.append(bag)\n",
    "    \n",
    "clinton_data = np.array([np.array([text, 0]) for text in clinton_data])\n",
    "trump_data = np.array([np.array([text, 1]) for text in trump_data])\n",
    "\n",
    "data = np.concatenate((clinton_data, trump_data), axis=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = sk_split(data[:, 0], data[:, 1], test_size = 0.25, random_state = 42)\n",
    "\n",
    "wordDictionary = {}\n",
    "wordCount = 0\n",
    "            \n",
    "for text in x_train:\n",
    "    wordCount = loadDictionary(text, wordCount)\n",
    "\n",
    "trainingBags = []\n",
    "testingBags = []\n",
    "for text in x_train:\n",
    "    bagOfWords(text, trainingBags, wordCount)\n",
    "    \n",
    "for text in x_test:\n",
    "    bagOfWords(text, testingBags, wordCount)\n",
    "\n",
    "model = linear_model.LogisticRegression()\n",
    "model.fit(trainingBags, y_train)\n",
    "print model.score(trainingBags, y_train)\n",
    "print model.score(testingBags, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.689116490792\n",
      "0.690564866543\n"
     ]
    }
   ],
   "source": [
    "def bagOfChars(text, dataset, cc):\n",
    "    bag = np.zeros(cc)\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(words): \n",
    "        if \"t.co\" in words[i]:\n",
    "            del words[i]\n",
    "        i += 1\n",
    "    \n",
    "    text = \" \".join(words)\n",
    "            \n",
    "    charCount = 0\n",
    "    for char in text:\n",
    "        index = None\n",
    "        if char.isalpha():\n",
    "            index = ord(char) - ord('a') + 1\n",
    "        elif char.isdigit():\n",
    "            index = ord(char) - ord('0') + 27\n",
    "        elif char == \" \":\n",
    "            index = 0\n",
    "                             \n",
    "        if index:\n",
    "            bag[index] += 1\n",
    "    \n",
    "    dataset.append(bag)\n",
    "\n",
    "clinton_data = np.array([np.array([text, 0]) for text in clinton_data])\n",
    "trump_data = np.array([np.array([text, 1]) for text in trump_data])\n",
    "\n",
    "data = np.concatenate((clinton_data, trump_data), axis=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = sk_split(data[:, 0], data[:, 1], test_size = 0.25, random_state = 42)\n",
    "\n",
    "charCount = 37\n",
    "\n",
    "trainingBags = []\n",
    "testingBags = []\n",
    "for text in x_train:\n",
    "    bagOfChars(text, trainingBags, charCount)\n",
    "    \n",
    "for text in x_test:\n",
    "    bagOfChars(text, testingBags, charCount)\n",
    "\n",
    "model = linear_model.LogisticRegression()\n",
    "model.fit(trainingBags, y_train)\n",
    "print model.score(trainingBags, y_train)\n",
    "print model.score(testingBags, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
