{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split as sk_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "np.set_printoptions(threshold='nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4833, 140, 37)\n",
      "(4833,)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('datasets/tweets.csv', delimiter=',')\n",
    "\n",
    "clinton_data = data[data.handle == \"HillaryClinton\"][\"text\"].as_matrix()\n",
    "trump_data = data[data.handle == \"realDonaldTrump\"][\"text\"].as_matrix()\n",
    "\n",
    "clinton_dataset = []\n",
    "trump_dataset = []\n",
    "\n",
    "def loadCharsFromTxt(text, dataset):\n",
    "    image = np.zeros((140, 37))\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(words): \n",
    "        if \"t.co\" in words[i]:\n",
    "            del words[i]\n",
    "        i += 1\n",
    "    \n",
    "    text = \" \".join(words)\n",
    "            \n",
    "    charCount = 0\n",
    "    for char in text:\n",
    "        index = None\n",
    "        if char.isalpha():\n",
    "            index = ord(char) - ord('a') + 1\n",
    "        elif char.isdigit():\n",
    "            index = ord(char) - ord('0') + 27\n",
    "        elif char == \" \":\n",
    "            index = 0\n",
    "                             \n",
    "        if index:\n",
    "            image[charCount, index] = 1\n",
    "            charCount += 1\n",
    "                    \n",
    "    dataset.append(image)\n",
    "\n",
    "for text in clinton_data:\n",
    "    loadCharsFromTxt(text, clinton_dataset)\n",
    "\n",
    "for text in trump_data:\n",
    "    loadCharsFromTxt(text, trump_dataset)\n",
    "    \n",
    "clinton_y = np.zeros(len(clinton_dataset))\n",
    "trump_y = np.full(len(trump_dataset), 1)\n",
    "\n",
    "x_data = np.concatenate((clinton_dataset, trump_dataset), axis=0)\n",
    "y_data = np.concatenate((clinton_y, trump_y), axis=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = sk_split(x_data, y_data, test_size = 0.25, random_state = 42)\n",
    "\n",
    "print x_train.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.977860542106\n",
      "0.891371818746\n"
     ]
    }
   ],
   "source": [
    "def loadDictionary(text, wc):\n",
    "    words = text.lower().split()\n",
    "    for word in words:\n",
    "        if \"t.co\" not in word:\n",
    "            i = 0\n",
    "            editedWord = word\n",
    "            while i < len(word):\n",
    "                if not word[i].isalpha() and not word[i].isdigit():\n",
    "                    editedWord = word[:i] + word[i+1:]\n",
    "                i += 1\n",
    "                \n",
    "            if word not in wordDictionary:\n",
    "                wordDictionary[word] = wc\n",
    "                wc += 1\n",
    "    \n",
    "    return wc\n",
    "                \n",
    "def bagOfWords(text, dataset, wc):\n",
    "    bag = np.zeros(wc)\n",
    "    for word in text.lower().split():\n",
    "        # ignore t.co links\n",
    "        if \"t.co\" not in word:\n",
    "            i = 0\n",
    "            \n",
    "#             ignore punctuation\n",
    "            editedWord = word\n",
    "            while i < len(word):\n",
    "                if not word[i].isalpha() and not word[i].isdigit():\n",
    "                    editedWord = word[:i] + word[i+1:]\n",
    "                i += 1\n",
    "\n",
    "            if editedWord in wordDictionary:\n",
    "                bag[wordDictionary[editedWord]] += 1\n",
    "    dataset.append(bag)\n",
    "    \n",
    "clintonData = np.array([np.array([text, 0]) for text in clinton_data])\n",
    "trumpData = np.array([np.array([text, 1]) for text in trump_data])\n",
    "\n",
    "data = np.concatenate((clintonData, trumpData), axis=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = sk_split(data[:, 0], data[:, 1], test_size = 0.25, random_state = 42)\n",
    "\n",
    "wordDictionary = {}\n",
    "wordCount = 0\n",
    "            \n",
    "for text in x_train:\n",
    "    wordCount = loadDictionary(text, wordCount)\n",
    "\n",
    "trainingBags = []\n",
    "testingBags = []\n",
    "for text in x_train:\n",
    "    bagOfWords(text, trainingBags, wordCount)\n",
    "    \n",
    "for text in x_test:\n",
    "    bagOfWords(text, testingBags, wordCount)\n",
    "\n",
    "model = linear_model.LogisticRegression()\n",
    "model.fit(trainingBags, y_train)\n",
    "print model.score(trainingBags, y_train)\n",
    "print model.score(testingBags, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.684667908132\n",
      "0.692116697703\n"
     ]
    }
   ],
   "source": [
    "def bagOfChars(text, dataset, cc):\n",
    "    bag = np.zeros(cc)\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(words): \n",
    "        if \"t.co\" in words[i]:\n",
    "            del words[i]\n",
    "        i += 1\n",
    "    \n",
    "    text = \" \".join(words)\n",
    "            \n",
    "    charCount = 0\n",
    "    for char in text:\n",
    "        index = None\n",
    "        if char.isalpha():\n",
    "            index = ord(char) - ord('a') + 1\n",
    "        elif char.isdigit():\n",
    "            index = ord(char) - ord('0') + 27\n",
    "        elif char == \" \":\n",
    "            index = 0\n",
    "                             \n",
    "        if index:\n",
    "            bag[index] += 1\n",
    "    \n",
    "    dataset.append(bag)\n",
    "\n",
    "clintonData = np.array([np.array([text, 0]) for text in clinton_data])\n",
    "trumpData = np.array([np.array([text, 1]) for text in trump_data])\n",
    "\n",
    "data = np.concatenate((clintonData, trumpData), axis=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = sk_split(data[:, 0], data[:, 1], test_size = 0.25, random_state = 42)\n",
    "\n",
    "charCount = 37\n",
    "\n",
    "trainingBags = []\n",
    "testingBags = []\n",
    "for text in x_train:\n",
    "    bagOfChars(text, trainingBags, charCount)\n",
    "    \n",
    "for text in x_test:\n",
    "    bagOfChars(text, testingBags, charCount)\n",
    "\n",
    "model = linear_model.LogisticRegression()\n",
    "model.fit(trainingBags, y_train)\n",
    "print model.score(trainingBags, y_train)\n",
    "print model.score(testingBags, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified 898 correctly out of 967; 0.928645294726 correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "# breaks text into words\n",
    "def text_split(text):\n",
    "    return re.findall('[a-z0-9]+', text)\n",
    "\n",
    "# train classifier with training data\n",
    "def train_classifier(clinton_count, trump_count):\n",
    "    priors = Counter()\n",
    "    priors['clinton'] = clinton_count\n",
    "    priors['trump'] = trump_count\n",
    "    \n",
    "    likelihood = defaultdict(Counter)\n",
    "    total_clinton_training_words = 0 \n",
    "    total_trump_training_words = 0\n",
    "    \n",
    "    for index, tweet in enumerate(x_train):\n",
    "        for word in tweet.lower().split():\n",
    "            # ignore t.co links\n",
    "            if \"t.co\" not in word:\n",
    "                i = 0\n",
    "\n",
    "                # ignore punctuation\n",
    "                editedWord = word\n",
    "                while i < len(word):\n",
    "                    if not word[i].isalpha() and not word[i].isdigit():\n",
    "                        editedWord = word[:i] + word[i+1:]\n",
    "                    i += 1\n",
    "\n",
    "                if int(y_train[index]) == 0:\n",
    "                    total_clinton_training_words += 1\n",
    "                    likelihood['clinton'][editedWord] += 1\n",
    "                else:\n",
    "                    total_trump_training_words += 1\n",
    "                    likelihood['trump'][editedWord] += 1\n",
    "\n",
    "    return(priors, likelihood)\n",
    "\n",
    "# classifier - uses priors & likelihood to calculate the posterior with test data  \n",
    "def classify_bayesian(tweet, priors, likelihood):\n",
    "    # return the class that maximizes the posterior\n",
    "    max_class = (-1E6, '')\n",
    "    for category in priors.keys():\n",
    "        # the number of articles in that category (used to estimate category frequency) \n",
    "        p = priors[category]\n",
    "\n",
    "        # calculate the total number of words for that category (positive or negative)\n",
    "        # used to estimate word frequency in a category\n",
    "        n = float(sum(likelihood[category].itervalues()))\n",
    "\n",
    "        # calculate the posterior by summing the products of word frequency and category \n",
    "        # frequency for each word\n",
    "        # 10,000 is an arbitrary value I found I could scale by such that the least number of\n",
    "        # \"infinite\" and \"0.0\" p-values would be printed and highest accuracy would occur. \n",
    "        for word in tweet.lower().split():\n",
    "            # ignore t.co links\n",
    "            if \"t.co\" not in word:\n",
    "                i = 0\n",
    "\n",
    "                # ignore punctuation\n",
    "                editedWord = word\n",
    "                while i < len(word):\n",
    "                    if not word[i].isalpha() and not word[i].isdigit():\n",
    "                        editedWord = word[:i] + word[i+1:]\n",
    "                    i += 1\n",
    "                    \n",
    "                p = 10000 * p * max(1E-10, likelihood[category][editedWord] / n)\n",
    "\n",
    "        if p > max_class[0]:\n",
    "            max_class = (p,category)\n",
    "    return max_class[1]\n",
    "\n",
    "clintonData = np.array([np.array([text, 0]) for text in clinton_data])\n",
    "trumpData = np.array([np.array([text, 1]) for text in trump_data])\n",
    "\n",
    "data = np.concatenate((clintonData, trumpData), axis=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = sk_split(data[:, 0], data[:, 1], test_size = 0.15, random_state = 42)\n",
    "\n",
    "trump_tweet_count = np.count_nonzero(y_train == '1')\n",
    "clinton_tweet_count = np.count_nonzero(y_train == '0')\n",
    "\n",
    "(priors, likelihood) = train_classifier(clinton_tweet_count, trump_tweet_count)\n",
    "\n",
    "num_correct = 0\n",
    "total_test_pages = len(x_test)\n",
    "\n",
    "for index, tweet in enumerate(x_test):\n",
    "    result = classify_bayesian(tweet, priors, likelihood)\n",
    "\n",
    "    if (result == 'clinton' and int(y_test[index]) == 0) or (result == 'trump' and int(y_test[index]) == 1):\n",
    "        num_correct += 1\n",
    "\n",
    "# report data\n",
    "print \"Classified {0} correctly out of {1}; {2} correct\\n\".format(num_correct, total_test_pages,\n",
    "    float(num_correct)/total_test_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
